{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6c30408f",
   "metadata": {},
   "source": [
    "# End-to End Satellite Image Segmention Pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "800d37fe",
   "metadata": {},
   "source": [
    "The end-to-end pipeline for satellite image segmentation is composed of the following steps:\n",
    "\n",
    "1. Data Ingestion\n",
    "2. Data Preprocessing\n",
    "3. Model Training\n",
    "4. Model Evaluation\n",
    "5. API Inference"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c42e1ae7",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bd0c8370",
   "metadata": {},
   "source": [
    "### Model training "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "43ecec5f",
   "metadata": {},
   "source": [
    "To Train the model locally, run:\n",
    "\n",
    "```bash\n",
    "python src/get_and_process_data.py\n",
    "```\n",
    "followed by:\n",
    "\n",
    "```bash\n",
    "python src/train_model.py\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "785030bf",
   "metadata": {},
   "source": [
    "Additionally, if you do not have a GPU, a colab notebook is available at `notebooks/train_model.ipynb`. The notebook pulls the repo from Github, runs the ingestion and training pipeline, then saves the model periodically to Google Drive. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "775e2445",
   "metadata": {},
   "source": [
    "### Model deployment via API"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0441b112",
   "metadata": {},
   "source": [
    "#### Local development environment "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ede939a4",
   "metadata": {},
   "source": [
    "1. Ensure that the `model_path` parameter in `conf/config.yaml` points to the right model path, and model source is set to `local`:\n",
    "\n",
    "\n",
    "    ```yaml\n",
    "    api:\n",
    "      MODEL_PATH: # model path to use for inference\n",
    "      MODEL_SOURCE: local\n",
    "    ```\n",
    "\n",
    "2. Ensure docker is installed on your local machine and service started\n",
    "\n",
    "3. Build the docker image\n",
    "\n",
    "    ```bash\n",
    "    docker build -t sat_img_seg -f .\\docker\\flask_app.dockerfile .\n",
    "    ```\n",
    "\n",
    "4. Run the docker image\n",
    "\n",
    "    ```bash\n",
    "    docker run -p 5000:5000 sat_img_seg\n",
    "    ```\n",
    "\n",
    "5. The API is now running on `http://localhost:5000/`\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7240cb04",
   "metadata": {},
   "source": [
    "#### Model deployment to AWS\n",
    "\n",
    "A github actions workflow is set up to automatically deploy the API to AWS EC2, whenever the `main` branch is updated.\n",
    "\n",
    "1. Upload trained model artifact, .pth, to google drive, and obtain the file id, the file id can be obtained from the shareable link : https://drive.google.com/file/<file_id>/view?usp=drive_link\n",
    "\n",
    "2. Spin up a EC2 instance on AWS, ensure that docker is installed and service started\n",
    "\n",
    "3. Make sure that you have a docker account on docker hub\n",
    "\n",
    "4. On the Github repo, ensure that the following secrets are set:\n",
    "    - DOCKER_USERNAME \n",
    "    - DOCKER_PASSWORD\n",
    "    - AWS_HOST (Public IP address of the EC2 instance)\n",
    "    - AWS_USERNAME (Username of the EC2 instance)\n",
    "    - AWS_PRIVATE_KEY (Private key of the EC2 instance, you should download a .pem file when you create the EC2 instance, copy and paste the content of the .pem file into the secret)\n",
    "\n",
    "5. Once a change has been made to the master branch, or manually triggered, the github actions workflow will be triggered, and the API will be deployed to the EC2 instance\n",
    "\n",
    "5. The API is now running on `http://<AWS_HOST>:5000/`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d56e9c4f",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "70d52292",
   "metadata": {},
   "source": [
    "The dataset consists of 1366 rural and 1156 urban satellite images and correpsonding masks consisting of remote sensing images from Nanjing, Changzhou, and Wuhan. They are part of the [LoveDA](https://github.com/Junjue-Wang/LoveDA) dataset. \n",
    "\n",
    "The masks contains 5 labels: unlablled (0), building (1), woodland (2), water(3), road(4). Unlablled class consists of all landcover types other than the types specified in labels 1-4. Overall, More than half of all mask pixels are unlablled, more than 1/3 are woodlands, and only a minority are buildings, water and road.\n",
    "\n",
    "The images and masks were split into 256x256 smaller images. Image augmentation where additional datasets were created by randomly varying the brightness and constrast of source images."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "089099ae",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "381ca95c",
   "metadata": {},
   "source": [
    "2 segmentation architectures were used. A basic encoder-decorder network and Unet (https://arxiv.org/pdf/1505.04597.pdf). The basic network achieved 0.72 of IOU (Intersection over Union) and the Unet achieved 0.74 after about 30-40 epoches trained on the Google Colab platform. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "876795e2",
   "metadata": {},
   "source": [
    "## Limitations and Future Exploration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d5fe0eda",
   "metadata": {},
   "source": [
    "### Limitations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "36647af5",
   "metadata": {},
   "source": [
    "- Does not handle overlapped objects well, i.e. tree over road\n",
    "- Does not pick up areas where separation between objects are not as apparent\n",
    "- Unlabelled class consists of grassland, concrete paving, shadow, etc.\n",
    "- Some ground truth labelling are inaccurate\n",
    "- Dataset only consists of some areas in Poland, mostly forested and rural areas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b34f4105",
   "metadata": {},
   "source": [
    "### Future explorations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3aaef38d",
   "metadata": {},
   "source": [
    "- Explore other architectures and pretrained models\n",
    "- Increase variety of datasets to include a fair representation of urban, rural, and different types of natural landcovers \n",
    "- Better labelled training sets\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "af835199",
   "metadata": {},
   "source": [
    "## Acknowledgement"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f83a8d38",
   "metadata": {},
   "source": [
    "The Unet architecture was based on the paper by Olaf Ronneberger, Philipp Fischer, and Thomas Brox: https://arxiv.org/pdf/1505.04597.pdf. \n",
    "\n",
    "In addition, the project takes reference from videos by DigitalSreeni: https://www.youtube.com/c/DigitalSreeni."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacf9c95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
